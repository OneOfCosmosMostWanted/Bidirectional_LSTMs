{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNcENlXUO7ej7kSUVZ+KEJC"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-2Mn9kiWJra"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Bidirectional, Dropout\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import yfinance as yf\n",
        "from textblob import TextBlob\n",
        "import tweepy\n",
        "\n",
        "# Step 1: Fetch Historical Stock Data using Yahoo Finance API\n",
        "def fetch_stock_data(ticker, start_date, end_date):\n",
        "    data = yf.download(ticker, start=start_date, end=end_date)\n",
        "    return data[['Close']]\n",
        "\n",
        "# Example: Fetching Google stock data\n",
        "stock_data = fetch_stock_data('GOOG', '2020-01-01', '2023-01-01')\n",
        "\n",
        "# Step 2: Preprocessing Stock Data\n",
        "def preprocess_stock_data(data, time_window):\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "    X, y = [], []\n",
        "    for i in range(time_window, len(scaled_data)):\n",
        "        X.append(scaled_data[i-time_window:i, 0])\n",
        "        y.append(scaled_data[i, 0])\n",
        "\n",
        "    return np.array(X), np.array(y), scaler\n",
        "\n",
        "time_window = 60\n",
        "X_stock, y_stock, scaler = preprocess_stock_data(stock_data.values, time_window)\n",
        "X_stock = np.reshape(X_stock, (X_stock.shape[0], X_stock.shape[1], 1))\n",
        "\n",
        "# Step 3: Fetch Tweets and Perform Sentiment Analysis using TextBlob\n",
        "def fetch_tweets(api_key, api_secret_key, access_token, access_token_secret, query):\n",
        "    auth = tweepy.OAuthHandler(api_key, api_secret_key)\n",
        "    auth.set_access_token(access_token, access_token_secret)\n",
        "    api = tweepy.API(auth)\n",
        "\n",
        "    tweets = tweepy.Cursor(api.search_tweets, q=query, lang=\"en\").items(100)\n",
        "    sentiments = []\n",
        "\n",
        "    for tweet in tweets:\n",
        "        analysis = TextBlob(tweet.text)\n",
        "        sentiments.append(analysis.sentiment.polarity)\n",
        "\n",
        "    return np.mean(sentiments)\n",
        "\n",
        "# Example: Replace with your Twitter API credentials and query\n",
        "api_key = \"your_api_key\"\n",
        "api_secret_key = \"your_api_secret_key\"\n",
        "access_token = \"your_access_token\"\n",
        "access_token_secret = \"your_access_token_secret\"\n",
        "sentiment_score = fetch_tweets(api_key, api_secret_key, access_token, access_token_secret, \"Google\")\n",
        "\n",
        "# Integrate sentiment score into the dataset (optional step if combining features)\n",
        "sentiment_feature = np.full((len(y_stock), 1), sentiment_score)\n",
        "X_combined = np.concatenate([X_stock[:, :, 0], sentiment_feature], axis=1).reshape(X_stock.shape[0], X_stock.shape[1], -1)\n",
        "\n",
        "# Step 4: Build Multi-Layer Bidirectional LSTM Model with TensorFlow/Keras\n",
        "model = Sequential([\n",
        "    Bidirectional(LSTM(50, return_sequences=True), input_shape=(X_combined.shape[1], X_combined.shape[2])),\n",
        "    Dropout(0.2),\n",
        "    Bidirectional(LSTM(50)),\n",
        "    Dropout(0.2),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "model.summary()\n",
        "\n",
        "# Step 5: Train the Model with Early Stopping and Dropout Regularization\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "\n",
        "history = model.fit(X_combined, y_stock,\n",
        "                    epochs=50,\n",
        "                    batch_size=32,\n",
        "                    validation_split=0.2,\n",
        "                    callbacks=[early_stopping])\n",
        "\n",
        "# Step 6: Evaluate the Model Performance\n",
        "y_pred_scaled = model.predict(X_combined)\n",
        "y_pred = scaler.inverse_transform(y_pred_scaled)\n",
        "y_actual = scaler.inverse_transform(y_stock.reshape(-1, 1))\n",
        "\n",
        "mae = mean_absolute_error(y_actual, y_pred)\n",
        "correlation = np.corrcoef(y_actual.flatten(), y_pred.flatten())[0][1]\n",
        "\n",
        "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
        "print(f\"Correlation Coefficient: {correlation}\")\n",
        "\n",
        "# Step 7: Visualize Predictions vs Actual Stock Prices\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(14, 7))\n",
        "plt.plot(y_actual.flatten(), label=\"Actual Prices\", color='blue')\n",
        "plt.plot(y_pred.flatten(), label=\"Predicted Prices\", color='red')\n",
        "plt.title(\"Stock Price Prediction\")\n",
        "plt.xlabel(\"Time\")\n",
        "plt.ylabel(\"Price\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ]
}